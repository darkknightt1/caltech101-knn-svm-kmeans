{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BO_j_7wdjPIZ"
   },
   "outputs": [],
   "source": [
    "#imported libs\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.utils import get_file\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Filtered images saved to: C:\\Users\\princ\\Downloads\\datasets\\caltech-101\\processed\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "dataset_dir = r\"C:\\Users\\princ\\Downloads\\datasets\\caltech-101\\caltech-101\\101_ObjectCategories\"\n",
    "output_dir = r\"C:\\Users\\princ\\Downloads\\datasets\\caltech-101\\processed\"  # Output directory for processed images\n",
    "kernel_size = (13, 13)  # Kernel size for Gaussian blur\n",
    "sigma = 2.0  # Standard deviation for Gaussian kernel\n",
    "\n",
    "# Function to apply Gaussian filter to an image\n",
    "def apply_gaussian_filter(image_path, kernel_size, sigma):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Read image in color\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at {image_path} could not be loaded.\")\n",
    "    blurred = cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "    return blurred\n",
    "\n",
    "# Process all images in the dataset\n",
    "def process_dataset(input_dir, output_dir, kernel_size, sigma):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_path = os.path.join(input_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            # Create corresponding output directory for this class\n",
    "            output_class_dir = os.path.join(output_dir, class_name)\n",
    "            os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "            for image_name in os.listdir(class_path):\n",
    "                input_image_path = os.path.join(class_path, image_name)\n",
    "                output_image_path = os.path.join(output_class_dir, image_name)\n",
    "\n",
    "                try:\n",
    "                    # Apply Gaussian filter\n",
    "                    processed_image = apply_gaussian_filter(input_image_path, kernel_size, sigma)\n",
    "\n",
    "                    # Save the processed image\n",
    "                    cv2.imwrite(output_image_path, processed_image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_image_path}: {e}\")\n",
    "\n",
    "# Apply Gaussian filter to the dataset\n",
    "process_dataset(dataset_dir, output_dir, kernel_size, sigma)\n",
    "\n",
    "print(\"Processing complete. Filtered images saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8677 files belonging to 100 classes.\n",
      "Using 6942 files for training.\n",
      "Found 8677 files belonging to 100 classes.\n",
      "Using 1735 files for validation.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = r\"C:\\Users\\princ\\Downloads\\datasets\\caltech-101\\processed\"\n",
    "\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "image_size = (200, 200)\n",
    "\n",
    "# Load the training and validation datasets\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "def dataset_to_numpy(dataset):\n",
    "    \"\"\"\n",
    "    Convert a tf.data.Dataset into NumPy arrays for features and labels.\n",
    "    Args:\n",
    "        dataset: A tf.data.Dataset object.\n",
    "    Returns:\n",
    "        X: Numpy array of features (images).\n",
    "        y: Numpy array of labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for images, labels in dataset:\n",
    "        X.append(images.numpy())\n",
    "        y.append(labels.numpy())\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y, axis=0)\n",
    "\n",
    "# Convert the train and validation datasets to NumPy arrays\n",
    "X_train, y_train = dataset_to_numpy(train_dataset)\n",
    "X_test, y_test = dataset_to_numpy(val_dataset)\n",
    "\n",
    "#print(f\"X_train shape: {X_train.shape}\")\n",
    "#print(f\"y_train shape: {y_train.shape}\")\n",
    "#print(f\"X_test shape: {X_test.shape}\")\n",
    "#print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class                Train      Validation\n",
      "--------------------------------------------------\n",
      "Faces                697        173       \n",
      "Leopards             172        28        \n",
      "Motorbikes           622        176       \n",
      "accordion            40         15        \n",
      "airplanes            637        163       \n",
      "anchor               35         7         \n",
      "ant                  37         5         \n",
      "barrel               36         11        \n",
      "bass                 40         14        \n",
      "beaver               30         16        \n",
      "binocular            31         2         \n",
      "bonsai               98         30        \n",
      "brain                81         17        \n",
      "brontosaurus         39         4         \n",
      "buddha               62         23        \n",
      "butterfly            73         18        \n",
      "camera               36         14        \n",
      "cannon               35         8         \n",
      "car_side             98         25        \n",
      "ceiling_fan          41         6         \n",
      "cellphone            50         9         \n",
      "chair                51         11        \n",
      "chandelier           84         23        \n",
      "cougar_body          32         15        \n",
      "cougar_face          55         14        \n",
      "crab                 54         19        \n",
      "crayfish             54         16        \n",
      "crocodile            38         12        \n",
      "crocodile_head       40         11        \n",
      "cup                  49         8         \n",
      "dalmatian            55         12        \n",
      "dollar_bill          40         12        \n",
      "dolphin              51         14        \n",
      "dragonfly            57         11        \n",
      "electric_guitar      57         18        \n",
      "elephant             51         13        \n",
      "emu                  43         10        \n",
      "euphonium            54         10        \n",
      "ewer                 67         18        \n",
      "ferry                52         15        \n",
      "flamingo             51         16        \n",
      "flamingo_head        38         7         \n",
      "garfield             28         6         \n",
      "gerenuk              31         3         \n",
      "gramophone           38         13        \n",
      "grand_piano          77         22        \n",
      "hawksbill            76         24        \n",
      "headphone            35         7         \n",
      "hedgehog             45         9         \n",
      "helicopter           68         20        \n",
      "ibis                 67         13        \n",
      "inline_skate         24         7         \n",
      "joshua_tree          52         12        \n",
      "kangaroo             72         14        \n",
      "ketch                94         20        \n",
      "lamp                 51         10        \n",
      "laptop               62         19        \n",
      "llama                63         15        \n",
      "lobster              34         7         \n",
      "lotus                56         10        \n",
      "mandolin             37         6         \n",
      "mayfly               32         8         \n",
      "menorah              70         17        \n",
      "metronome            28         4         \n",
      "minaret              59         17        \n",
      "nautilus             43         12        \n",
      "octopus              27         8         \n",
      "okapi                30         9         \n",
      "pagoda               40         7         \n",
      "panda                23         15        \n",
      "pigeon               39         6         \n",
      "pizza                42         11        \n",
      "platypus             30         4         \n",
      "pyramid              50         7         \n",
      "revolver             71         11        \n",
      "rhino                51         8         \n",
      "rooster              40         9         \n",
      "saxophone            31         9         \n",
      "schooner             50         13        \n",
      "scissors             29         10        \n",
      "scorpion             72         12        \n",
      "sea_horse            43         14        \n",
      "snoopy               31         4         \n",
      "soccer_ball          51         13        \n",
      "stapler              38         7         \n",
      "starfish             71         15        \n",
      "stegosaurus          44         15        \n",
      "stop_sign            54         10        \n",
      "strawberry           27         8         \n",
      "sunflower            67         18        \n",
      "tick                 41         8         \n",
      "trilobite            69         17        \n",
      "umbrella             63         12        \n",
      "watch                194        45        \n",
      "water_lilly          28         9         \n",
      "wheelchair           45         14        \n",
      "wild_cat             28         6         \n",
      "windsor_chair        43         13        \n",
      "wrench               23         16        \n",
      "yin_yang             52         8         \n"
     ]
    }
   ],
   "source": [
    "def count_images_per_class(dataset):\n",
    "    class_counts = defaultdict(int)\n",
    "    class_names = dataset.class_names  # Get class names\n",
    "    for images, labels in dataset:\n",
    "        for label in labels.numpy():\n",
    "            class_counts[class_names[label]] += 1\n",
    "    return class_counts\n",
    "\n",
    "# Get class-wise counts\n",
    "train_class_counts = count_images_per_class(train_dataset)\n",
    "val_class_counts = count_images_per_class(val_dataset)\n",
    "\n",
    "# Display the results\n",
    "print(f\"{'Class':<20} {'Train':<10} {'Validation':<10}\")\n",
    "print(\"-\" * 50)\n",
    "all_classes = sorted(set(train_class_counts.keys()).union(val_class_counts.keys()))\n",
    "for class_name in all_classes:\n",
    "    train_count = train_class_counts.get(class_name, 0)\n",
    "    val_count = val_class_counts.get(class_name, 0)\n",
    "    print(f\"{class_name:<20} {train_count:<10} {val_count:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "def accuracy(y_test1, y_pred1):\n",
    "    y_pred1 = np.array(y_pred1)\n",
    "    counter = 0\n",
    "    for i in range(len(y_pred1)):\n",
    "      if (y_pred1[i] == y_test1[i]):\n",
    "        counter += 1\n",
    "    accuracy = counter / len(y_pred1)\n",
    "    accuracy *= 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIVBeUWWjoEE"
   },
   "outputs": [],
   "source": [
    "#Color Histogran Extraction def\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    \"\"\"\n",
    "    Extract a 3D color histogram from an RGB image.\n",
    "    Args:\n",
    "        image (numpy array): Input image in RGB format.\n",
    "        bins (tuple): Number of bins for each channel (R, G, B).\n",
    "    Returns:\n",
    "        numpy array: Flattened color histogram feature vector.\n",
    "    \"\"\"\n",
    "    # Calculate the 3D histogram for the HSV channels\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, bins, [0, 255, 0, 256, 0, 256])\n",
    "    # Normalize the histogram to ensure invariance to lighting changes\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ic306d_1xb-h"
   },
   "outputs": [],
   "source": [
    "#HOG def\n",
    "def extract_hog_features(image):\n",
    "    normalized_image = image /255.0\n",
    "    channels = cv2.split(normalized_image)\n",
    "    # HOG parameters\n",
    "    winSize = (32, 32)\n",
    "    blockSize = (8, 8)\n",
    "    blockStride = (8, 8)\n",
    "    cellSize = (8, 8)\n",
    "    nbins = 9\n",
    "    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "    # Initialize a list to hold HOG features for each channel\n",
    "    concatenated_hog_features = []\n",
    "    \n",
    "    for channel in channels:\n",
    "        # Ensure the channel is grayscale and of type uint8\n",
    "        channel = (channel * 255).astype(np.uint8) if channel.dtype == np.float32 else channel\n",
    "        hog_features = hog.compute(channel)\n",
    "        concatenated_hog_features.append(hog_features.flatten())\n",
    "\n",
    "    # Concatenate features from all channels into a single feature vector\n",
    "    concatenated_hog_features = np.concatenate(concatenated_hog_features)\n",
    "    return concatenated_hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i70Yk1IzmN8f"
   },
   "outputs": [],
   "source": [
    "#LBP def\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "def extract_lbp_features(image, num_points=32, radius=8):\n",
    "    #normalized_image = image /255.0\n",
    "    # Split the image into Red, Green, and Blue channels\n",
    "    channels = cv2.split(image)\n",
    "    # Initialize a list to store concatenated LBP histograms for all channels\n",
    "    concatenated_hist = []\n",
    "    \n",
    "    # Loop through each channel\n",
    "    for channel in channels:\n",
    "        # Compute LBP representation for the channel\n",
    "        lbp = local_binary_pattern(channel, num_points, radius, method='uniform')\n",
    "        # Calculate the histogram of LBP\n",
    "        (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, num_points + 3), range=(0, num_points + 2))\n",
    "        # Normalize the histogram\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-7)\n",
    "        \n",
    "        # Append the histogram to the concatenated list\n",
    "        concatenated_hist.extend(hist)\n",
    "    \n",
    "    # Convert the concatenated list to a numpy array and return\n",
    "    return np.array(concatenated_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FyTP7LLSz97f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\princ\\anaconda3\\envs\\MVEnv\\Lib\\site-packages\\skimage\\feature\\texture.py:360: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract LBP features for train and test\n",
    "lbp_features_train = np.array([extract_lbp_features(image) for image in X_train])\n",
    "lbp_features_test  = np.array([extract_lbp_features(image) for image in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1fXjorG5-uP",
    "outputId": "c71caa0a-3376-411e-83b0-b6f23973de83"
   },
   "outputs": [],
   "source": [
    "# Step 2: Extract HOG features for train and test\n",
    "hog_features_train = np.array([extract_hog_features(image) for image in X_train])\n",
    "hog_features_test  = np.array([extract_hog_features(image) for image in X_test])\n",
    "#reduce the features\n",
    "from sklearn.feature_selection import SelectKBest, f_classif \n",
    "selector = SelectKBest(score_func=f_classif, k=400)\n",
    "hog_features_train = selector.fit_transform(hog_features_train, y_train)\n",
    "hog_features_test = selector.transform(hog_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract Color Histogram features for train and test\n",
    "clhg_features_train = np.array([extract_color_histogram(image) for image in X_train])\n",
    "clhg_features_test  = np.array([extract_color_histogram(image) for image in X_test])\n",
    "#reduce the features\n",
    "from sklearn.feature_selection import SelectKBest, f_classif \n",
    "selector2 = SelectKBest(score_func=f_classif, k=180)\n",
    "hog_features_train = selector2.fit_transform(hog_features_train, y_train)\n",
    "hog_features_test = selector2.transform(hog_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k = 3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def count_occurrences(self,input_array,distances):\n",
    "        output_array = []\n",
    "        for i in range(len(input_array)):\n",
    "            count = sum(\n",
    "                np.array_equal(input_array[i], other)\n",
    "                for other in input_array\n",
    "            )\n",
    "            output_array.append(count)\n",
    "        return output_array.index(max(output_array))  \n",
    "            #output_array.append((1/count)*distances[i])\n",
    "        #return output_array.index(min(output_array))  # return index of minimum count (1/count * distance)'''\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(self, image_test):\n",
    "        distances = np.linalg.norm(self.X_train - image_test.reshape(1,-1), axis=1)\n",
    "        k_nearest = np.argsort(distances)[:self.k]\n",
    "        #print(k_nearest)\n",
    "        k_nearest_labels = self.y_train[k_nearest]\n",
    "        #print(k_nearest_labels)\n",
    "        prediction = self.count_occurrences(k_nearest_labels,k_nearest)\n",
    "        \n",
    "        \n",
    "        return np.array(k_nearest_labels[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.178674351585016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare with pre-built KNN from sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(lbp_features_train, y_train)\n",
    "predictions = knn.predict(lbp_features_test)\n",
    "accuracy(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNNClassifier(9)\n",
    "model.fit(lbp_features_train,y_train)\n",
    "\n",
    "y_pred = np.array([model.predict( i.reshape(1, -1)) for i in  lbp_features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.5821325648415"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KNNClassifier(9)\n",
    "model2.fit(hog_features_train,y_train)\n",
    "\n",
    "y_pred2 = np.array([model2.predict( i.reshape(1, -1)) for i in  hog_features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.322766570605186"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = KNNClassifier(9)\n",
    "model3.fit(clhg_features_train,y_train)\n",
    "  \n",
    "y_pred3 = np.array([model3.predict( i.reshape(1, -1)) for i in clhg_features_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.680115273775215"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\princ\\anaconda3\\envs\\MVEnv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [399 471] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\princ\\anaconda3\\envs\\MVEnv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.27089337175792"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN for the three features concatenated together , gives higher accuracy\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "combined_features = np.concatenate     ([clhg_features_train*0.3, lbp_features_train*1.1], axis=1)\n",
    "combined_features_test = np.concatenate([clhg_features_test*0.3, lbp_features_test *1.1], axis=1)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=450)\n",
    "selected_features = selector.fit_transform(combined_features, y_train)\n",
    "selected_features_test = selector.transform(combined_features_test)\n",
    "\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=9,weights = \"distance\")\n",
    "knn2.fit(selected_features, y_train)\n",
    "predictionss = knn2.predict(selected_features_test)\n",
    "accuracy(y_test, predictionss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=101, max_iter=300, tol=1e-6):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol  # Tolerance for convergence\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Step 1: Randomly initialize centroids\n",
    "        np.random.seed(42)\n",
    "        self.centroids = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]\n",
    "        print(X.shape[0])\n",
    "        print (self.centroids.shape)\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Step 2: Assign points to the nearest cluster\n",
    "            self.labels = self._assign_clusters(X)\n",
    "\n",
    "            # Step 3: Compute new centroids\n",
    "            new_centroids = np.array([  X[self.labels == i].mean(axis=0) for i in range(self.n_clusters)  ])\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.all(np.abs(new_centroids - self.centroids) < self.tol):\n",
    "                print(f\"Converged at iteration {iteration}\")\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def _assign_clusters(self, X):\n",
    "        '''My implementation , faster implementation than chatgpt implementation\n",
    "        distances=[]\n",
    "        for i in range (len(X) ):\n",
    "            distances.append(     np.linalg.norm(self.centroids - X[i] , axis=1)    ) \n",
    "        return np.argmin(  np.array(distances) , axis = 1  )'''\n",
    "    \n",
    "        # Compute distances from each point to each centroid\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2) #(distances.shape) 7316 x 100 , 100 euclidean distances for each img in the data set\n",
    "        return np.argmin(distances, axis=1)  # Assign each point to the nearest centroid\n",
    "\n",
    "    def predict(self, X): \n",
    "        X = np.array(X).flatten()\n",
    "        # Compute the Euclidean distances between the test case and all centroids\n",
    "        distances = np.linalg.norm(self.centroids - X, axis=1)   # (100 x no. of features) - (1 X no.of features)   , broadcasting occurs = (100 X ,)\n",
    "        return np.argmin(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6942\n",
      "(101, 180)\n",
      "Converged at iteration 42\n"
     ]
    }
   ],
   "source": [
    "model_K_mean = KMeans()\n",
    "model_K_mean.fit(hog_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy = test_accuracy(model_K_mean, hog_features_test , y_test)\\nprint(f\"Clustering Accuracy: {accuracy:.2f}%\")'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "def test_accuracy(model, X_test, y_test):\n",
    "    # Step 1: Predict the cluster for each test data point\n",
    "    predicted_clusters = np.array([model.predict(x.reshape(1, -1)) for x in X_test])\n",
    "\n",
    "    # Step 2: Map clusters to true labels (majority class in each cluster)\n",
    "    cluster_labels = np.zeros(model.n_clusters)  # Array to hold the true label for each cluster\n",
    "    \n",
    "    for cluster in range(model.n_clusters): # 0 ---- 101\n",
    "        # Get the indices of all points assigned to this cluster\n",
    "        cluster_points = y_test[predicted_clusters == cluster]  \n",
    "        \n",
    "        # Assign the most common true label in this cluster\n",
    "        if len(cluster_points) > 0:\n",
    "            most_common_label = mode(cluster_points)[0]\n",
    "            cluster_labels[cluster] = most_common_label\n",
    "\n",
    "    # Step 3: Predict labels for each test case\n",
    "    predicted_labels = np.array([cluster_labels[cluster] for cluster in predicted_clusters])\n",
    "    \n",
    "    # Step 4: Calculate accuracy\n",
    "    accuracy = np.sum(predicted_labels == y_test) / len(y_test) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "'''accuracy = test_accuracy(model_K_mean, hog_features_test , y_test)\n",
    "print(f\"Clustering Accuracy: {accuracy:.2f}%\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy with sklearn's KMeans: 27.61%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Step 4: Perform K-means clustering\n",
    "kmeanss = KMeans(n_clusters=101, random_state=42)  # Choose number of clusters\n",
    "kmeanss.fit(features_3d)\n",
    "labels = kmeanss.labels_\n",
    "\n",
    "\n",
    "def test_accuracy_sklearn_kmeans(model, X_test, y_test):\n",
    "    # Step 1: Predict the cluster for each test data point\n",
    "    predicted_clusters = model.predict(X_test)\n",
    "\n",
    "    # Step 2: Map clusters to true labels (majority class in each cluster)\n",
    "    cluster_labels = np.zeros(model.n_clusters)  # Array to hold the true label for each cluster\n",
    "    \n",
    "    for cluster in range(model.n_clusters):\n",
    "        # Get the indices of all points assigned to this cluster\n",
    "        cluster_points = y_test[predicted_clusters == cluster]\n",
    "        \n",
    "        # Assign the most common true label in this cluster\n",
    "        if len(cluster_points) > 0:\n",
    "            most_common_label = mode(cluster_points)[0]  # Get the most common label in this cluster\n",
    "            cluster_labels[cluster] = most_common_label\n",
    "\n",
    "    # Step 3: Predict labels for each test case\n",
    "    predicted_labels = np.array([cluster_labels[cluster] for cluster in predicted_clusters])\n",
    "    \n",
    "    # Step 4: Calculate accuracy\n",
    "    accuracy_test = np.sum(predicted_labels == y_test) / len(y_test) * 100\n",
    "    return accuracy_test\n",
    "accuracy_test2 = test_accuracy_sklearn_kmeans(kmeanss, Test_features_3d_X , y_test)\n",
    "print(f\"Clustering Accuracy with sklearn's KMeans: {accuracy_test2:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, param_grid=None, cv=3):\n",
    "        \"\"\"\n",
    "        Initialize the SVM classifier with hyperparameter tuning.\n",
    "        Args:\n",
    "            param_grid (dict): Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values.\n",
    "            cv (int): Number of cross-validation folds for `GridSearchCV`.\n",
    "        \"\"\"\n",
    "        # Default parameter grid if none is provided\n",
    "        if param_grid is None:\n",
    "            self.param_grid = {\n",
    "                'kernel': ['linear', 'rbf', 'poly'],  # Kernel types to try\n",
    "                'C': [0.1, 1, 10],                  # Regularization strength values\n",
    "                'gamma': ['scale', 'auto']          # Gamma values (only for rbf, poly, sigmoid)\n",
    "            }\n",
    "        else:\n",
    "            self.param_grid = param_grid\n",
    "\n",
    "        self.cv = cv\n",
    "        self.grid_search = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the SVM model on the training data using GridSearchCV.\n",
    "        Args:\n",
    "            X_train (numpy array): Feature vectors for training data.\n",
    "            y_train (numpy array): Labels for training data.\n",
    "        \"\"\"\n",
    "        # Perform grid search to find the best hyperparameters\n",
    "        self.grid_search = GridSearchCV(\n",
    "            SVC(),\n",
    "            self.param_grid,\n",
    "            cv=self.cv,\n",
    "            scoring='accuracy',\n",
    "            verbose=1  # Display progress\n",
    "        )\n",
    "        self.grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Print the best parameters and score\n",
    "        print(\"Best parameters:\", self.grid_search.best_params_)\n",
    "        print(\"Best cross-validation accuracy:\", self.grid_search.best_score_)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the labels for the test data.\n",
    "        Args:\n",
    "            X_test (numpy array): Feature vectors for test data.\n",
    "        Returns:\n",
    "            numpy array: Predicted labels.\n",
    "        \"\"\"\n",
    "        if self.grid_search is None:\n",
    "            raise ValueError(\"Model is not trained yet. Call `fit` before prediction.\")\n",
    "        \n",
    "        best_model = self.grid_search.best_estimator_  # Get the best model from the grid search\n",
    "        return best_model.predict(X_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data and print a classification report.\n",
    "        Args:\n",
    "            X_test (numpy array): Feature vectors for test data.\n",
    "            y_test (numpy array): True labels for test data.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Best cross-validation accuracy: 0.35177182368193605\n"
     ]
    }
   ],
   "source": [
    "modelx = SVMClassifier()\n",
    "modelx.fit(lbp_features_train,y_train)\n",
    "\n",
    "y_predx = np.array([modelx.predict( i.reshape(1, -1)) for i in  lbp_features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.65706051873199"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_predx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best cross-validation accuracy: 0.2582829155862864\n"
     ]
    }
   ],
   "source": [
    "modely = SVMClassifier()\n",
    "modely.fit(hog_features_train,y_train)\n",
    "\n",
    "y_predy = np.array([modely.predict( i.reshape(1, -1)) for i in  hog_features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.37752161383285"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_predy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best cross-validation accuracy: 0.3573898012100259\n"
     ]
    }
   ],
   "source": [
    "modelz = SVMClassifier()\n",
    "modelz.fit(clhg_features_train,y_train)\n",
    "\n",
    "y_predz = np.array([modelz.predict( i.reshape(1, -1)) for i in clhg_features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.694524495677236"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_predz)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
